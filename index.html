<!DOCTYPE html>
<html lang="en">
<head>
  <title>Language-Mediated, Object-Centric Representation Learning</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script> -->

  <link rel="stylesheet" href="./resources/bootstrap.min.css">
  <script src="./resources/jquery.min.js"></script>
  <script src="./resources/bootstrap.min.js"></script>
  <style type="text/css">
    body{font-family:"Open Sans",Segoe,"Segoe UI","Lucida Sans Unicode","Lucida Grande","Avenir","Seravek","Ubuntu","DejaVu Sans","Trebuchet MS",Verdana,Arial,sans-serif}
    #header{background:#eee;opacity:0.95;margin:0 auto;padding:50px 0 0;text-align:center;cursor:default;text-align:center}
    #header-container{margin:0 auto;padding: 0 2em;max-width:1600px}
    
    #header-container .col-logo{text-align:left}
    #header-container .logo{position:relative;z-index:100;height:50px;margin-top:20px;margin-right:10px}
    
    #header-container .col-info{text-align:left;margin-bottom:20px}
    #header-container #title{margin:.525em 0 1.525em;font-size:1.6em;font-weight:800;text-align:center}
    
    .paper-info{display:inline-block;margin:0 auto;text-align:left}
    .paper-info-margin{margin-bottom:20px}
    .paper-info p, .paper-info h3{line-height:1.6em; margin-bottom: 0em}
    .paper-info .title{font-size:16px;color:#333333;font-weight:600}
    .paper-info .authors, .paper-info .authors a{color:#666666}
    .paper-info .email{color:#666; font-size:14px}
    .paper-info .tag{margin:auto auto;padding:0;list-style:none;text-align:left}
    .paper-info .tag li{display:inline-block;margin:auto;padding:0 3px 0 0;line-height:10px}
    .paper-info .conference, .paper-info .conference a{color:#BB2222;font-weight:600}
    .paper-info .note{display:block;color:#666666;text-decoration:none;font-size:14px}
    
    #wave-canvas{display:block;margin:-80px 0 0;width:100%;height:150px}
    
    #content{padding-top:0px;text-align:left}
    #content-container{margin:0 auto;padding: 0 2em;max-width:1600px;color:#333333}
    
    #content-container .header{background:#eee;padding:15px 30px 5px;border-bottom:3px solid #dddddd}
    #content-container .header .indicator{color:#777777;margin-right:12px}
    
    #content-container .content{background:#ffffff;padding:15px 5px 15px 30px}
    #content-container .content .caption{color:#666666}
    #content-container .content .bib{color:#666666;font-size:14px;}
    #content-container .content .bib pre{margin:0;padding:0;}
    
    #footer{padding:2em 0 0.5em;margin:30px 0 0;background:#ffffff;opacity:0.95;font-size:14px;line-height:12px;text-align:center}
    .highlight, .highlight a{color:#BB2222;font-weight:600}
        </style>
</head>
<body>

<div class="jumbotron text-center">
  <h1>Language-Mediated, Object-Centric Representation Learning</h1>
  <h4>
    <a href="https://2021.aclweb.org/">
    ACL 2021 Findings
    </a>
  </h4>
  <hr>
  <h5>
    <a href="http://lang-orl.github.io">
      Ruocheng Wang
    </a>*<sup>1</sup>
    &nbsp;&nbsp;
    <a href="http://jiayuanm.com">
      Jiayuan Mao
    </a>*<sup>2</sup>
    &nbsp;&nbsp;
    <a href="https://gershmanlab.com/people/sam.html">
      Samuel J. Gershman
    </a>†<sup>3</sup>
    &nbsp;&nbsp;
    <a href="http://jiajunwu.com">
      Jiajun Wu
    </a>†<sup>1</sup>
    <br>
  </h5>
  <div>
    <sup>1</sup>Stanford University 
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <sup>2</sup>MIT CSAIL 
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <sup>3</sup>Harvard University

  </div>
  <div>
  * and † indicate equal contribution <br>
  </div>

  <br>
  <h5>
    <a href="https://arxiv.org/pdf/2012.15814.pdf">
      [Paper]
    </a>
    <a href="./resources/wang2021lorl.bib">
      [BibTeX]
    </a>
  </h5>
  <!-- <p>Resize this responsive page to see the effect!</p>  -->
</div>
  
<div class="container">
    <!-- <div class="row"> -->
      <h2 class="text-left">Abstract</h2>
      <hr>
      <!-- <hr> -->
    <!-- </div> -->
    <p>
        We present Language-mediated, Object-centric  Representation  Learning (LORL), a paradigm for learning disentangled, object-centric scene representations from vision and language. LORL builds upon recent advances in unsupervised object segmentation, notably MONet and Slot Attention. While these algorithms learn an object-centric representation just by reconstructing the input image, LORL enables them to further learn to associate the learned representations to concepts, i.e., words for object categories, properties, and spatial relationships, from language input. These object-centric concepts derived from language facilitate the learning of object-centric representations. LORL can be integrated with various unsupervised segmentation algorithms that are language-agnostic. Experiments show that the integration of LORL consistently improves the object segmentation performance of MONet and Slot Attention on two datasets via the help of language. We also show that concepts learned by LORL, in conjunction with segmentation algorithms such as MONet, aid downstream tasks such as referring expression comprehension.
    </p>
  </div><br><br>


<div class="container bg-3">    
    <!-- <div class="row"> -->
      <h2 class="text-left">Motivation</h2>
      <hr>
      <div class="col-sm-12 text-center">
        <img src="./resources/teaser.png" class="img-fluid center" style="width:auto" alt="Image">
      </div>
    <!-- </div> -->
    <br>
    <p>Two illustrative cases of Language-mediated, Object-centric Representation Learning. Different colors in the segmentation masks indicate individual objects recognized by the model. LORL can learn from visual and language inputs to associate various concepts: <i> black</i>, <i> pan</i>, <i>leg</i> with the visual appearance of individual objects. Furthermore, language provides cues about how an input scene should be segmented into individual objects: (a) segmenting the frying pan and its handle into two parts yields an incorrect answer to the question (Segmentation II); (b) an incorrect parsing of the chair image makes the counting result wrong (Segmentation II)
    </p>
  </div><br><br>

<div class="container bg-3">    
    <!-- <div class="row"> -->
      <h2 class="text-left">Model</h2>
      <hr>
      <div class="col-sm-12 text-center">
        <img src="./resources/model.png" class="img-responsive mx-auto" style="width:100%" alt="Image">
      </div>
    <!-- </div> -->
    <br>
  </div><br><br>

<div class="container bg-3">
  <h2 class="text-left">Related Publication</h2>
  <hr>
  <div class="paper-info paper-info-margin">
      <h3 class="title">
      The Neuro-Symbolic Concept Learner:
      Interpreting Scenes, Words, and Sentences From Natural Supervision</h3>
      <p class="authors">
          <a href="http://jiayuanm.com/">Jiayuan Mao</a>,
          <a href="http://people.csail.mit.edu/ganchuang/">Chuang Gan</a>,
          <a href="https://sites.google.com/site/pushmeet/">Pushmeet Kohli</a>,
          <a href="http://cocosci.mit.edu/josh">Joshua B. Tenenbaum</a>, and
          <a href="http://jiajunwu.com/">Jiajun Wu</a>
      </p>
      <ul class="tag">
          <li class="conference"><a href="https://nips.cc/Conferences/2018">ICLR 2019</a> (Oral)</li>
          <li><a href="http://nscl.csail.mit.edu/data/papers/2019ICLR-NSCL.pdf">Paper</a> /</li>
          <li><a href="http://nscl.csail.mit.edu">Project Page</a> /</li>
          <li><a href="http://nscl.csail.mit.edu/data/bibtex/2019ICLR-NSCL.bib">BibTeX</a></li>
          <li class="authors"></li>
      </ul>
  </div>

</div>

</body>
</html>
